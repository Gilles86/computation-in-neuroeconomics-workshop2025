{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba736957",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd38848e",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e0748e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('resources/estimation_data.tsv', sep='\\t', index_col=[0,1,2,3])\n",
    "# Group by subject and n, then calculate mean response\n",
    "grouped = df.groupby(['subject', 'n'])['response'].mean().reset_index()\n",
    "\n",
    "# Plot one line per subject, all in the same color\n",
    "sns.relplot(\n",
    "    x='n',\n",
    "    y='response',\n",
    "    units='subject',  # This ensures one line per subject, but same color\n",
    "    estimator=None,   # Required when using units\n",
    "    data=grouped,\n",
    "    kind='line',\n",
    "    color='k',        # All lines will be black\n",
    "    legend=False,\n",
    "    alpha=.25,\n",
    ")\n",
    "plt.axline((10, 10), slope=1, color='r', linestyle='--')\n",
    "\n",
    "plt.title('Plot of average responses')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12df689c",
   "metadata": {},
   "source": [
    "## Step 1\n",
    "**1. Define the Generative Model**\n",
    "- **Encoding function:** $f(x) = x^\\alpha$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0534fc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "n_r_grid = 1000\n",
    "alpha = .1  # Tuning parameter\n",
    "nu = .1     # Noise standard deviation\n",
    "\n",
    "@tf.function\n",
    "def to_sensory_space(x, alpha=0.5, x_min=10., x_max=40.):\n",
    "    r_min = x_min**alpha\n",
    "    r_max = x_max**alpha\n",
    "    r = x**alpha\n",
    "    r = (r - r_min) / (r_max - r_min)  # Normalize to [0, 1]\n",
    "    return r\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe4ed12",
   "metadata": {},
   "source": [
    " - **Noise model:** $r \\sim \\mathcal{N}(f(x_0), \\nu^2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6164e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate normal distributions\n",
    "def p_r_given_x0(x0, r_grid, alpha, nu, truncate=True):\n",
    "    mu = to_sensory_space(x0, alpha=alpha)  # Shape: (n_x0,)\n",
    "    # Reshape for broadcasting: mu (n_x0, 1), r_grid (n_r_grid,)\n",
    "    mu = tf.expand_dims(mu, 1)  # Shape: (n_x0, 1)\n",
    "    r_grid_expanded = tf.expand_dims(r_grid, 0)  # Shape: (1, n_r_grid)\n",
    "    # Create a Normal distribution for each mu\n",
    "\n",
    "    if truncate:\n",
    "        # Truncate the distribution to [0, 1]\n",
    "        dist = tfp.distributions.TruncatedNormal(loc=mu, scale=nu, low=0., high=1.)\n",
    "    else:\n",
    "        dist = tfp.distributions.Normal(loc=mu, scale=nu)\n",
    "\n",
    "    # Evaluate PDF at r_grid points\n",
    "    p = dist.prob(r_grid_expanded)  # Shape: (n_x0, n_r_grid)\n",
    "    return p\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db6f374",
   "metadata": {},
   "source": [
    "**2. Build the Likelihood Grid**\n",
    "- Create a grid of possible $x_0$ and $r$ values\n",
    "- Make a $p(r | x_0)$ for each pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3580f54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stimulus values and sensory grid\n",
    "x0 = tf.range(10.0, 41.0, dtype=tf.float32)\n",
    "r_grid = tf.linspace(0.0, 1.0, n_r_grid)\n",
    "\n",
    "likelihood = p_r_given_x0(x0, r_grid, alpha, nu, truncate=True)\n",
    "plt.imshow(likelihood.numpy().T, extent=(10, 40, 0, 1), aspect='auto', origin='lower', cmap='viridis')\n",
    "plt.ylabel('Sensory Space (r)')\n",
    "plt.xlabel('Stimulus Value (x0)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57ebafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = pd.DataFrame(likelihood.numpy(), index=pd.Index(x0.numpy(), name='x0'),\n",
    "                  columns=pd.Index(r_grid.numpy(), name='r'))\n",
    "ll.loc[[10, 20, 30, 40], :].T.plot()\n",
    "sns.despine()\n",
    "\n",
    "plt.xlabel('r')\n",
    "plt.ylabel('p(r | x0)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d28442",
   "metadata": {},
   "source": [
    "\n",
    "**3. Bayesian Inference**\n",
    "- For each observed $r$, compute the posterior:\n",
    "  $$p(x|r) = \\frac{p(r|x)p(x)}{p(r)}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03641ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xhat_given_r(x0, likelihood):\n",
    "    p_x0 = tf.ones_like(x0) / tf.cast(tf.size(x0), tf.float32)  # Uniform prior\n",
    "    p_r = tf.reduce_sum(likelihood * p_x0[:, tf.newaxis], axis=0)  # Marginal likelihood p(r)\n",
    "    p_x0_given_r = (likelihood * p_x0[:, tf.newaxis]) / p_r  # Posterior p(x0|r)\n",
    "    # Expected value of x0 given r\n",
    "    hat_x = tf.reduce_sum(x0[:, tf.newaxis] * p_x0_given_r, axis=0)  # Shape: (n_r_grid,)\n",
    "\n",
    "    return hat_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e0ff20",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d9c31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = tf.linspace(0.0, 1.0, n_r_grid)\n",
    "\n",
    "x_hat = get_xhat_given_r(x0, likelihood)\n",
    "plt.plot(r, x_hat)\n",
    "plt.xlabel('r')\n",
    "plt.ylabel('x_hat')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766cc341",
   "metadata": {},
   "source": [
    "- Estimate $\\hat{x}$ as the expected value:\n",
    "  $$\\hat{x} = \\mathbb{E}[x|r]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40acb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def get_p_x_hat_given_x0(hat_x, possible_x_hats ,likelihood, normalize=True, dx=1.):\n",
    "\n",
    "    \"\"\"\n",
    "    returns p(x_hat | x_0) (shape: (n_x0, n_x_hat))\n",
    "    \"\"\"\n",
    "    \n",
    "    x_hat_edges = tf.concat([[possible_x_hats[0] - dx / 2.],\n",
    "                             (possible_x_hats[:-1] + possible_x_hats[1:]) / 2.,\n",
    "                             [possible_x_hats[-1] + dx / 2.]], axis=0)\n",
    "\n",
    "    def get_distribution_over_xhat(ll):\n",
    "        return tfp.stats.histogram(hat_x,\n",
    "                            edges=x_hat_edges,\n",
    "                            weights=ll)\n",
    "\n",
    "    # x_0 x x_hat\n",
    "    p_xhat_given_x0 = tf.vectorized_map(get_distribution_over_xhat, likelihood) \n",
    "\n",
    "    if normalize:\n",
    "        p_xhat_given_x0 = p_xhat_given_x0 / tf.reduce_sum(p_xhat_given_x0, axis=0, keepdims=True)\n",
    "\n",
    "        # Replace nans with 0s\n",
    "        p_xhat_given_x0 = tf.where(tf.math.is_nan(p_xhat_given_x0), tf.zeros_like(p_xhat_given_x0), p_xhat_given_x0)\n",
    "\n",
    "    return p_xhat_given_x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1230fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_grid = tf.linspace(0.0, 1.0, n_r_grid)\n",
    "alpha = 1.\n",
    "nu = .1\n",
    "\n",
    "likelihood = p_r_given_x0(x0, r_grid, alpha, nu)\n",
    "hat_x = get_xhat_given_r(x0, likelihood)\n",
    "x_hat_given_x0 = get_p_x_hat_given_x0(hat_x, tf.range(10.0, 41.0, dtype=tf.float32), likelihood)\n",
    "\n",
    "plt.imshow(x_hat_given_x0.numpy().T, extent=(10, 40, 10, 40), aspect='auto', origin='lower', cmap='viridis')\n",
    "plt.xlabel('x0')\n",
    "plt.ylabel('x_hat')\n",
    "plt.plot([10, 40], [10, 40], color='r', linestyle='--')\n",
    "plt.colorbar(label='p(x_hat | x0)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae251c32",
   "metadata": {},
   "source": [
    "**4. Data Likelihood Function**\n",
    "- Define a function that returns the response distribution over $\\hat{x}$ for any $x_0$: $p(\\hat{x} | x_0)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c19f39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def data_likelihood(n, response, alpha, nu, n_r_grid=1000):\n",
    "    r_grid = tf.linspace(0.0, 1.0, n_r_grid)\n",
    "    # possible_x_hats, _ = tf.unique(response)\n",
    "    possible_x_hats = tf.range(10.0, 41.0, dtype=tf.float32)\n",
    "\n",
    "    likelihood = p_r_given_x0(x0, r_grid, alpha, nu)\n",
    "    hat_x = get_xhat_given_r(x0, likelihood)\n",
    "    x_hat_given_x0 = get_p_x_hat_given_x0(hat_x, possible_x_hats, likelihood)\n",
    "\n",
    "    # get the indices of the values in n and response in possible_x_hats\n",
    "    n_indices = tf.searchsorted(possible_x_hats, n)\n",
    "    response_indices = tf.searchsorted(possible_x_hats, response)\n",
    "    combined_indices = tf.stack([n_indices, response_indices], axis=1)\n",
    "\n",
    "    # Get the likelihood of the data\n",
    "    data_ll = tf.gather_nd(x_hat_given_x0, combined_indices)\n",
    "    # Avoid log(0) by adding a small constant\n",
    "    data_ll = tf.math.log(data_ll + 1e-10)\n",
    "    return tf.reduce_sum(data_ll)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f8aa88",
   "metadata": {},
   "source": [
    "# Vectorize the grid search using tf.vectorized_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3032f0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Vectorize for grid search\n",
    "@tf.function\n",
    "def grid_search(n, response, grid_alpha, grid_nu):\n",
    "    def compute_ll(params):\n",
    "        alpha, nu = params[0], params[1]\n",
    "        return data_likelihood(n, response, alpha, nu)\n",
    "    \n",
    "    param_grid = tf.stack(tf.meshgrid(grid_alpha, grid_nu), axis=-1)\n",
    "    param_grid = tf.reshape(param_grid, (-1, 2))\n",
    "    ll_values = tf.vectorized_map(compute_ll, param_grid)\n",
    "    return param_grid, ll_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57b334b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select data\n",
    "data = df[~df.isnull().any(axis=1)].loc[1].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffe9c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_alpha = 20\n",
    "n_nu = 20\n",
    "grid_alpha = tf.linspace(0.1, 1.0, n_alpha)\n",
    "grid_nu = tf.linspace(0.01, .3, n_nu)\n",
    "\n",
    "\n",
    "pars, ll = grid_search(data['n'].values, data['response'].values, grid_alpha, grid_nu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c03473",
   "metadata": {},
   "outputs": [],
   "source": [
    "How fast can you evaluate 400 differnet p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24dd9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(ll.numpy().reshape(n_alpha, n_nu), extent=(0.01, 1.0, 0.1, 1.0), origin='lower', aspect='auto', cmap='viridis')\n",
    "plt.colorbar(label='Log Likelihood')\n",
    "plt.xlabel('nu')\n",
    "plt.ylabel('alpha')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecac9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best fitting parameters:\n",
    "best_idx = tf.argmax(ll)\n",
    "best_params = pars[best_idx]\n",
    "best_alpha, best_nu = best_params[0].numpy(), best_params[1].numpy()\n",
    "print(f'Best alpha: {best_alpha}, Best nu: {best_nu}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae72aa4c",
   "metadata": {},
   "source": [
    "## Assignment\n",
    "\n",
    "We have now code that can fit the two-parameter model using grid search.\n",
    "\n",
    "Can you now:\n",
    "\n",
    " * Plot the model's predictions for the x_hat versus the actual data.\n",
    " * What about the border conditions $x_0 = 10$ or $x_0 = 40$. What do you notice?\n",
    " * How could you improve the model to solve this issue?\n",
    " * What happens when you reduce the grid size over r? Which tradeoff is to be made here?\n",
    "\n",
    " * Fit the model to all subjects and plot predictions and distributions over parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e5150c",
   "metadata": {},
   "source": [
    "Let's try to further imrpove model fit using gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ab62a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "# Initialize trainable variables\n",
    "alpha = tf.Variable(0.1, dtype=tf.float32, name='alpha')\n",
    "nu = tf.Variable(0.1, dtype=tf.float32, name='nu')\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "# Data\n",
    "n = tf.constant(data['n'].values, dtype=tf.float32)\n",
    "response = tf.constant(data['response'].values, dtype=tf.float32)\n",
    "\n",
    "# Gradient descent loop\n",
    "for step in range(100):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Compute loss\n",
    "        loss = -data_likelihood(n, response, alpha, nu)\n",
    "\n",
    "    # Compute gradients\n",
    "    gradients = tape.gradient(loss, [alpha, nu])\n",
    "\n",
    "    # Check gradients\n",
    "    if any(g is None for g in gradients):\n",
    "        print(\"No gradients for some variables!\")\n",
    "        print(\"Gradients:\", gradients)\n",
    "        break\n",
    "\n",
    "    # Apply gradients\n",
    "    optimizer.apply_gradients(zip(gradients, [alpha, nu]))\n",
    "\n",
    "    if step % 10 == 0:\n",
    "        print(f\"Step {step}, Loss: {loss.numpy():.4f}, alpha: {alpha.numpy():.4f}, nu: {nu.numpy():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3168d4c1",
   "metadata": {},
   "source": [
    "What is the issue here? How can we solve this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22d218d",
   "metadata": {},
   "source": [
    "Implement a model with a different transfer function, e.g. $f(x) = \\log x$ (standard approach in numerosity research). Which model fits better?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c101636d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "soglio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
